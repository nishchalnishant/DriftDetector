# Automated Data Setup Guide

## ğŸ¯ Overview

This project includes **automated data ingestion** - no manual data downloads or uploads required!

## ğŸš€ Quick Start (Recommended)

### Option 1: Complete Automation

```bash
# Install dependencies
pip install -r src/training/requirements.txt

# Run complete pipeline (downloads data + trains model)
python src/quick_start.py
```

**This single command**:
- âœ… Downloads 876K+ sensor readings from Microsoft Azure
- âœ… Processes and engineers 32 features
- âœ… Trains Isolation Forest model
- âœ… Exports to ONNX format
- âœ… Ready for deployment!

### Option 2: Step-by-Step

#### 1. Install Dependencies

```bash
pip install -r src/training/requirements.txt
```

#### 2. Download and Prepare Data

```bash
cd src
python -c "
from data.ingestion import DataIngestionPipeline

pipeline = DataIngestionPipeline()
train_df, test_df = pipeline.prepare_training_data()
print(f'âœ… Ready! Training: {len(train_df):,} samples')
"
```

#### 3. Train Model

```bash
python training/train.py
```

---

## ğŸ“Š Dataset Details

### Microsoft Azure Predictive Maintenance Dataset

**Source**: Azure ML Sample Experiments (Public)  
**License**: Open source, free to use  
**Download**: Automatic via HTTPS  

**Specifications**:
- **Records**: 876,000+ telemetry readings
- **Machines**: 100 industrial machines
- **Duration**: January 2015 - January 2016 (1 year)
- **Frequency**: Hourly sensor readings
- **Failures**: 761 failure events
- **Size**: ~50MB (compressed)

**Sensors**:
1. **Voltage** - Operating voltage levels
2. **Rotational Speed** - RPM measurements
3. **Pressure** - Operating pressure
4. **Vibration** - Vibration amplitude

**Machine Metadata**:
- Model type (4 models)
- Age in months
- Operating history

**Labels**:
- Normal operation: 90-95%
- Pre-failure (24h window): 5-10%

---

## ğŸ“ Generated Files

After running data ingestion:

```
data/
â”œâ”€â”€ raw/                         # Downloaded CSV files
â”‚   â”œâ”€â”€ telemetry.csv           # ~60MB - Sensor readings
â”‚   â”œâ”€â”€ errors.csv              # Error logs
â”‚   â”œâ”€â”€ maint.csv               # Maintenance records
â”‚   â”œâ”€â”€ machines.csv            # Machine metadata
â”‚   â””â”€â”€ failures.csv            # Failure events
â”‚
â””â”€â”€ processed/                   # Ready for training
    â”œâ”€â”€ train_data.parquet      # 80% train split
    â”œâ”€â”€ test_data.parquet       # 20% test split
    â”œâ”€â”€ sensor_data.parquet     # Complete dataset
    â””â”€â”€ reference.parquet       # Drift detection baseline
```

---

## ğŸ”§ Features Generated

### Base Features (4)
- `voltage`
- `rotational_speed`
- `pressure`
- `vibration`

### Rolling 1-Hour Features (16)
For each sensor:
- `{sensor}_mean_1h`
- `{sensor}_std_1h`
- `{sensor}_min_1h`
- `{sensor}_max_1h`

### Rolling 24-Hour Features (12)
For each sensor:
- `{sensor}_mean_24h`
- `{sensor}_std_24h`
- `{sensor}_trend_24h`

**Total: 32 features**

---

## ğŸ“ Usage Examples

### Basic Usage

```python
from src.data.ingestion import DataIngestionPipeline

# Initialize
pipeline = DataIngestionPipeline(
    data_dir="data/raw",
    processed_dir="data/processed"
)

# Download and process
train_df, test_df = pipeline.prepare_training_data(
    test_size=0.2,
    save_to_disk=True
)

print(f"Training samples: {len(train_df):,}")
print(f"Features: {train_df.shape[1]}")
print(f"Anomaly rate: {train_df['is_anomaly'].mean():.2%}")
```

### Get Feature Names

```python
features = pipeline.get_feature_names()
print(f"Total features: {len(features)}")
print(features)
```

### Custom Processing

```python
# Fetch raw data
telemetry = pipeline.fetch_azure_predictive_maintenance_data()

# Apply custom transformations
telemetry['power'] = telemetry['voltage'] * telemetry['rotational_speed']

# Create rolling features
df = pipeline.create_rolling_features(telemetry)
```

---

## ğŸ”„ Alternative Datasets

The ingestion module supports multiple datasets. To switch:

### Option 1: Modify Source

Edit `src/data/ingestion.py`:

```python
class DataIngestionPipeline:
    # Change to your preferred dataset
    CUSTOM_URL = "https://your-dataset-url.com/data.csv"
    
    def fetch_custom_data(self):
        # Add your custom logic
        pass
```

### Option 2: Provide Custom Data

```bash
# Skip auto-download, use your own data
python training/train.py --data_path /path/to/your/data.parquet
```

**Required columns**:
- `machine_id` (string)
- `event_timestamp` (datetime)
- Sensor columns (float)
- `is_anomaly` (int, 0 or 1) - optional

---

## ğŸ› Troubleshooting

### Issue: Download Fails

```bash
# Check internet connection
ping azuremlsampleexperiments.blob.core.windows.net

# Retry with verbose output
python -c "
from src.data.ingestion import DataIngestionPipeline
import logging
logging.basicConfig(level=logging.DEBUG)

pipeline = DataIngestionPipeline()
pipeline.prepare_training_data()
"
```

### Issue: Out of Memory

```python
# Process in smaller batches
pipeline = DataIngestionPipeline()

# Reduce window size
DRIFT_WINDOW_SIZE = 500  # Instead of 1000
```

### Issue: Slow Processing

```bash
# Check available resources
python -c "
import psutil
print(f'CPU cores: {psutil.cpu_count()}')
print(f'RAM: {psutil.virtual_memory().total / 1e9:.1f} GB')
"

# Processing 876K records typically takes 2-5 minutes
```

---

## âœ… Verification

After data ingestion, verify:

```bash
# Check files exist
ls -lh data/processed/

# Validate data
python -c "
import pandas as pd

train = pd.read_parquet('data/processed/train_data.parquet')
print(f'âœ“ Training samples: {len(train):,}')
print(f'âœ“ Features: {len(train.columns)}')
print(f'âœ“ Anomalies: {train[\"is_anomaly\"].sum():,}')
print(f'âœ“ Time range: {train[\"event_timestamp\"].min()} to {train[\"event_timestamp\"].max()}')
"
```

Expected output:
```
âœ“ Training samples: 700,000+
âœ“ Features: 35+
âœ“ Anomalies: 40,000+
âœ“ Time range: 2015-01-01 to 2015-10-01
```

---

## ğŸ’¡ Benefits

âœ… **Zero manual work** - Fully automated  
âœ… **Real-world data** - Not synthetic  
âœ… **Production-ready** - Same format as deployed system  
âœ… **Reproducible** - Same data every time  
âœ… **Well-documented** - Clear feature engineering  
âœ… **Scalable** - Handles large datasets efficiently  

---

## ğŸ“š References

- [Microsoft Azure ML Samples](https://github.com/Azure/MachineLearningNotebooks)
- [Predictive Maintenance Overview](https://learn.microsoft.com/en-us/azure/architecture/industries/manufacturing/predictive-maintenance-overview)
- [Dataset Documentation](https://gallery.azure.ai/Experiment/Predictive-Maintenance-Step-1-of-3-data-preparation-and-feature-engineering-2)

---

**Ready to train? Just run `python src/quick_start.py` ğŸš€**
